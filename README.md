# Semantic Compression: Reconstructing Photorealistic Images from Human-Understandable Representations

## Introduction

This project is an implementation of semantic image compression. It aims to move away from storing raw pixels and instead extracts and stores a human-understandable semantic representation of an image. We propose an encoder-decoder architecture that leverages segmentation maps for strong structural constraints and text descriptions for content guidance. Through a controllable diffusion process, this architecture can reconstruct a new, photorealistic image that is semantically consistent with the original.

## Environment Setup

```bash
pip install -r requirements.txt
```

> [!NOTE]
> This project was developed and tested for multi-GPU training. Support for single-GPU setups has not been verified.

## Dataset Preparation

This project supports standard datasets like VOC2012. The main requirement is that the images are in RGB format and contain a prominent subject.

1.  Download your dataset.
2.  Place the dataset in the `data/` directory.
3.  The expected directory structure is as follows:

```
my_controlnet/
├── data/
│   ├── original_images/  (Rename your images, e.g., "0_image.png", "1_image.png")
│   │   ├── train/
│   │   ├── test/
│   │   └── val/
│   └── segmented_images/  (Generated by `process_input.py`)
│       ├── train/
│       ├── test/
│       └── val/
└── ...
```

## Training

**1. Run Semantic Encoding**

First, process your input images to generate segmentation maps and text prompts. The `process_input.py` script utilizes the `ImageEncoder` class for this.

```bash
python process_input.py \
    --input_dir [PATH_TO_YOUR_INPUT_IMAGES] \
    --output_dir [PATH_TO_SAVE_ENCODED_RESULTS] \
    --segmentation_ckpt [PATH_TO_YOUR_SEGMENTATION_MODEL]
```

**2. Start training**
You can train the model using the following command. Remember to adjust the paths and parameters in your YAML config file first.

```bash
python train.py \
    --config_path ./configs/config.yaml \
    --sd_model_path [PATH_TO_YOUR_SD_MODEL.ckpt] \
    --gpus 0 1 \
    --batch_size_per_gpu [YOUR_BATCH_SIZE]
```

## Inference

**1. Run Semantic Encoding**

Be sure you have prepared the segmentation maps and text prompts. If not, you can run `process_input.py` script to do so.

**2. Run Decoding (Image Generation)**

Next, use the generated control signals (segmentation maps and prompts) to reconstruct the images.

```bash
python process_decoding.py \
    --config_path [PATH_TO_YOUR_CONFIG.yaml] \
    --checkpoint_path [PATH_TO_YOUR_TRAINED_MODEL.ckpt] \
    --input_path [PATH_TO_FOLDER_WITH_CONTROL_IMAGES] \
    --output_dir [PATH_TO_SAVE_GENERATED_IMAGES] \
    --prompt_json_path [OPTIONAL_PATH_TO_PROMPT.json] \
    --gpu_ids "0,1" \
    --use_fp16
```

## Evaluation

You can use `compress_images.py` to compress original images to a target file size(kb) for baseline comparison (e.g., JPEG, PNG, WebP).

```bash
python compress_images.py \
    --input-dir ./data/original_images/val/ \
    --output-dir ./compressed_images/val \
    --target-size 5
```

To evaluate the model's performance with standard metrics, use `evaluate.py`:

```bash
python evaluate.py \
    --prediction_path ./generated_images \
    --ground_truth_path ./data/original_images/val
```

## Pre-trained Models

Download links and instructions for pre-trained models are provided below.

| Model                | Location                                            |
| -------------------- | --------------------------------------------------- |
| Segmentation Model   | `checkpoints/best_deeplabv3plus_resnet101_voc_os16.pth` |
| Decoder Model        | `checkpoints/final.ckpt`                            |

> [!IMPORTANT]
> The pre-trained models listed above are compatible with the Stable Diffusion 1.5 base model. If you wish to use a different version of Stable Diffusion, you may need to retrain the decoder.

## Acknowledgements

This project is based on or inspired by the following outstanding work:
*   [ControlNet](https://github.com/lllyasviel/ControlNet)
*   [diffusers](https://github.com/huggingface/diffusers)

We thank the authors of these projects for their significant contributions to the community.

